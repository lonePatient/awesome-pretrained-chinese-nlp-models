# Awesome Pretrained Chinese NLP Models[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­ï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆPretrained Language Modelsï¼‰å·²æˆä¸ºéå¸¸é‡è¦çš„åŸºç¡€æŠ€æœ¯ï¼Œæœ¬ä»“åº“ä¸»è¦æ”¶é›†ç›®å‰ç½‘ä¸Šå…¬å¼€çš„ä¸€äº›é«˜è´¨é‡ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹(æ„Ÿè°¢åˆ†äº«èµ„æºçš„å¤§ä½¬)ï¼Œå¹¶å°†æŒç»­æ›´æ–°......

**æ³¨**: ğŸ¤—[huggingface](https://github.com/huggingface/transformers)æ¨¡å‹ä¸‹è½½åœ°å€: 1. [huggingfaceå®˜æ–¹åœ°å€](https://huggingface.co/models)

# Expand Table of Contents

+ [NLUç³»åˆ—](#NLUç³»åˆ—)
  - [BERT](#BERT)
  - [RoBERTa](#RoBERTa)
  - [ALBERT](#ALBERT)
  - [NEZHA](#NEZHA)
  - [XLNET](#XLNET)
  - [MacBERT](#MacBERT)
  - [WoBERT](#WoBERT)
  - [ELECTRA](#ELECTRA)
  - [ZEN](#ZEN)
  - [ERNIE](#ERNIE)
  - [ERNIE3](#ERNIE3)
  - [RoFormer](#RoFormer)
  - [StructBERT](#StructBERT)
  - [Lattice-BERT](#Lattice-BERT)
  - [Mengzi-BERT](#Mengzi-BERT)
  - [ChineseBERT](#ChineseBERT)
  - [TaCL](#TaCL) 
  - [MC-BERT](#MC-BERT)
  - [äºŒéƒç¥](#äºŒéƒç¥)
  - [PERT](#PERT)
  - [MobileBERT](#MobileBERT)
  - [GAU-Î±](#GAU-Î±)
+ [NLGç³»åˆ—](#NLGç³»åˆ—)
  - [GPT](#GPT)
  - [GPT-3](#GPT-3)
  - [NEZHA-GEN](#NEZHA-GEN)
  - [CPM-Generate](#CPM-Generate)
  - [T5](#T5)
  - [T5-PEGASUS](#T5-PEGASUS)
  - [Mengzi-T5](#Mengzi-T5)
  - [ç›˜å¤Î±](#PanGu-Alpha)
  - [EVA](#EVA)
  - [BART](#BART)
  - [é—»ä»²](#é—»ä»²)
  - [ä½™å…ƒ](#ä½™å…ƒ)
  - [RWKV](#RWKV)
+ [NLU-NLGç³»åˆ—](#NLU-NLGç³»åˆ—)
  - [UniLM](#UniLM)
  - [Simbert](#Simbert)
  - [RoFormer-sim](#RoFormer-sim)
  - [CPM-2](#CPM-2)
  - [CPT](#CPT)
  - [å‘¨æ–‡ç‹](#å‘¨æ–‡ç‹)
  - [GLM](#GLM)
+ [Multi-Modal](#Multi-Modal)
  - [WenLan](#WenLan)
  - [CogView](#CogView)
  - [ç´«ä¸œå¤ªåˆ](#ç´«ä¸œå¤ªåˆ)
  - [Mengzi-oscar](#Mengzi-oscar)
  - [R2D2](#R2D2)
  - [Chinese-CLIP](#Chinese-CLIP)
+ [Table](#Table)
  - [SDCUP](#SDCUP)

+ [æ›´æ–°æ—¥å¿—](#æ›´æ–°)

## NLUç³»åˆ—

### BERT

+ 2018 | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | Jacob Devlin, et al. | arXiv | [`PDF`](https://arxiv.org/abs/1810.04805)
+ 2019 | Pre-Training with Whole Word Masking for Chinese BERT | Yiming Cui, et al. | arXiv | [`PDF`](https://arxiv.org/abs/1906.08101)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| BERT-Base | base | [Google Drive](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip) | | [Google Research](https://github.com/google-research) | [github](https://github.com/google-research/bert)| é€šç”¨ |
| BERT-wwm | base | <p>[Google Drive](https://drive.google.com/open?id=1RoTQsXp2hkQ1gSRVylRIJfQxJUgkfJMW)<br>[è®¯é£äº‘-07Xj](http://pan.iflytek.com/#/link/A2483AD206EF85FD91569B498A3C3879)</p> | [Google Drive](https://drive.google.com/open?id=1AQitrjbvCWc51SYiLN-cJq4e0WiNN4KY) | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/Chinese-BERT-wwm) | é€šç”¨ |
| BERT-wwm-ext | base | <p>[Google Drive](https://drive.google.com/open?id=1buMLEjdtrXE2c4G1rpsNGWEx7lUQ0RHi)<br>[è®¯é£äº‘-4cMG](http://pan.iflytek.com/#/link/653637473FFF242C3869D77026C9BDB5)</p> | [Google Drive](https://drive.google.com/open?id=1iNeYFhCBJWeUsIlnW_2K6SMwXkM4gLb_) | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/Chinese-BERT-wwm) | é€šç”¨ |
| bert-base-æ°‘äº‹  | base | [é˜¿é‡Œäº‘](https://thunlp.oss-cn-qingdao.aliyuncs.com/bert/ms.zip) | | [THUNLP](https://github.com/thunlp) | [github](https://github.com/thunlp/OpenCLaP) | å¸æ³• |
| bert-base-åˆ‘äº‹ | base| [é˜¿é‡Œäº‘](https://thunlp.oss-cn-qingdao.aliyuncs.com/bert/xs.zip) | | [THUNLP](https://github.com/thunlp) | [github](https://github.com/thunlp/OpenCLaP) | å¸æ³• |
| BAAI-JDAI-BERT |base | [äº¬ä¸œäº‘](https://jdai009.s3.cn-north-1.jdcloud-oss.com/jd-aig/open/models/nlp_baai/20190918/JDAI-BERT.tar.gz?AWSAccessKeyId=86FD402DD0CB693D629F6E62DC11E0E6&Expires=1634304356&Signature=dfnLvSzXeerRPSnMsZXSoEwdmkw%3D) | | [JDAI](https://github.com/jd-aig)  | [github](https://github.com/jd-aig/nlp_baai/tree/master/pretrained_models_and_embeddings) | ç”µå•†å®¢æœå¯¹è¯ |
| FinBERT | base | <p>[Google Drive](https://drive.google.com/file/d/193B4sT63mMeh4zfge0FJbbFY447KiJXp/view?usp=sharing)<br>[ç™¾åº¦ç½‘ç›˜-1cmp](https://pan.baidu.com/share/init?surl=D-pVJyW6bbJSre5RxotJkA)</p> | <p>[Google Drive](https://drive.google.com/file/d/1qW1YWtw3q9Q28QThrIY-rDU9Gl-SLIKO/view?usp=sharing)<br>[ç™¾åº¦ç½‘ç›˜-986f](https://pan.baidu.com/share/init?surl=y_O586GBmZZ7g4d2nOF0Vg)</p> | [Value Simplex](https://github.com/valuesimplex) | [github](https://github.com/valuesimplex/FinBERT) | é‡‘èç§‘æŠ€é¢†åŸŸ |
| EduBERT |base | [å¥½æœªæ¥AI](https://ai.100tal.com/download/TAL-EduBERT-TF.zip)  | [å¥½æœªæ¥AI](https://ai.100tal.com/download/TAL-EduBERT.zip) | [tal-tech](https://github.com/tal-tech) | [github](https://github.com/tal-tech/edu-bert) | æ•™è‚²é¢†åŸŸ |
| guwenbert-base  |base | |<p>[ç™¾åº¦ç½‘ç›˜-4jng](https://pan.baidu.com/s/1dw_08p7CVsz0jVj4jd58lQ)<br>[huggingface](https://huggingface.co/ethanyt/guwenbert-base)</p> | [Ethan](https://github.com/Ethan-yt) | [github](https://github.com/Ethan-yt/guwenbert)  | å¤æ–‡é¢†åŸŸ |
| guwenbert-large |large | | <p>[ç™¾åº¦ç½‘ç›˜-m5sz](https://pan.baidu.com/s/1TL9mBIlIv2rSvp61xCkeJQ)<br>[huggingface](https://huggingface.co/ethanyt/guwenbert-large)</p> | [Ethan](https://github.com/Ethan-yt) | [github](https://github.com/Ethan-yt/guwenbert) | å¤æ–‡é¢†åŸŸ  |
| BERT-CCPoem | small | | [thunlp](https://thunlp.oss-cn-qingdao.aliyuncs.com/BERT_CCPoem_v1.zip) | [THUNLP-AIPoet](https://github.com/THUNLP-AIPoet) | [github](https://github.com/THUNLP-AIPoet/BERT-CCPoem) | å¤å…¸è¯—æ­Œ  |

å¤‡æ³¨: 

> wwmå…¨ç§°ä¸º**Whole Word Masking **,ä¸€ä¸ªå®Œæ•´çš„è¯çš„éƒ¨åˆ†WordPieceå­è¯è¢«maskï¼Œåˆ™åŒå±è¯¥è¯çš„å…¶ä»–éƒ¨åˆ†ä¹Ÿä¼šè¢«mask

> extè¡¨ç¤ºåœ¨æ›´å¤šæ•°æ®é›†ä¸‹è®­ç»ƒ

### ChineseBERT

+ 2021 | ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information | Zijun Sun, et al. | arXiv | [`PDF`](https://arxiv.org/pdf/2106.16038.pdf)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| ChineseBERT |base | | [huggingface](https://huggingface.co/ShannonAI/ChineseBERT-base) | [ShannonAI](https://github.com/ShannonAI) | [github](https://github.com/ShannonAI/ChineseBert) | é€šç”¨  |
|ChineseBERT |large | | [huggingface](https://huggingface.co/ShannonAI/ChineseBERT-large) | [ShannonAI](https://github.com/ShannonAI) | [github](https://github.com/ShannonAI/ChineseBert) | é€šç”¨  |

### RoBERTa

+ 2019 | RoBERTa: A Robustly Optimized BERT Pretraining Approach | Yinhan Liu, et al. | arXiv | [`PDF`](https://arxiv.org/pdf/1907.11692.pdf)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| RoBERTa-tiny-clue  | tiny  |[Google Drive](https://storage.googleapis.com/cluebenchmark/pretrained_models/RoBERTa-tiny-clue.zip) | [ç™¾åº¦ç½‘ç›˜-8qvb](https://pan.baidu.com/share/init?surl=hoR01GbhcmnDhZxVodeO4w) | [CLUE](https://github.com/CLUEbenchmark)    | [github](https://github.com/CLUEbenchmark/CLUEPretrainedModels) | é€šç”¨ |
| RoBERTa-tiny-pair | tiny |  [google drive](https://storage.googleapis.com/cluebenchmark/pretrained_models/RoBERTa-tiny-pair.zip) | [ç™¾åº¦ç½‘ç›˜-8qvb](https://pan.baidu.com/share/init?surl=hoR01GbhcmnDhZxVodeO4w) | [CLUE](https://github.com/CLUEbenchmark)  | [github](https://github.com/CLUEbenchmark/CLUEPretrainedModels) | é€šç”¨ |
| RoBERTa-tiny3L768-clue | tiny  |  [Google Drive](https://storage.googleapis.com/cluebenchmark/pretrained_models/RoBERTa-tiny3L768-clue.zip) |  | [CLUE](https://github.com/CLUEbenchmark) | [github](https://github.com/CLUEbenchmark/CLUEPretrainedModels) | é€šç”¨ |
| RoBERTa-tiny3L312-clue | tiny  |  [google drive](https://storage.googleapis.com/cluebenchmark/pretrained_models/RoBERTa-tiny3L312-clue.zip) | [ç™¾åº¦ç½‘ç›˜-8qvb](https://pan.baidu.com/share/init?surl=hoR01GbhcmnDhZxVodeO4w) | [CLUE](https://github.com/CLUEbenchmark) | [github](https://github.com/CLUEbenchmark/CLUEPretrainedModels) | é€šç”¨ |
| RoBERTa-large-pair | large |  [Google Drive](https://storage.googleapis.com/cluebenchmark/pretrained_models/RoBERTa-large-pair.zip) | [ç™¾åº¦ç½‘ç›˜-8qvb](https://pan.baidu.com/share/init?surl=hoR01GbhcmnDhZxVodeO4w) | [CLUE](https://github.com/CLUEbenchmark)    | [github](https://github.com/CLUEbenchmark/CLUEPretrainedModels) | é€šç”¨ |
| RoBERTa-large-clue | large  |  [google drive](https://storage.googleapis.com/cluebenchmark/pretrained_models/RoBERTa-large-clue.zip) | [ç™¾åº¦ç½‘ç›˜-8qvb](https://pan.baidu.com/share/init?surl=hoR01GbhcmnDhZxVodeO4w) | [CLUE](https://github.com/CLUEbenchmark)    | [github](https://github.com/CLUEbenchmark/CLUEPretrainedModels) | é€šç”¨ |
| RBT3 | 3å±‚base|  <p>[Google Drive](https://drive.google.com/file/d/1-rvV0nBDvRCASbRz8M9Decc3_8Aw-2yi/view?usp=drive_open)<br>[è®¯é£äº‘-b9nx](https://pan.iflytek.com/link/275E5B46185C982D4AF5AC295E1651B6)</p> | [Google Drive](https://drive.google.com/file/d/1_LqmIxm8Nz1Abvlqb8QFZaxYo-TInOed/view) | [Yiming Cui](https://github.com/ymcui)      | [github](https://github.com/ymcui/Chinese-BERT-wwm) | é€šç”¨ |
| RBTL3 | 3å±‚large|  <p>[Google Drive](https://drive.google.com/open?id=1Jzn1hYwmv0kXkfTeIvNT61Rn1IbRc-o8)<br>[è®¯é£äº‘-vySW](https://pan.iflytek.com/link/0DD18FAC080BAF75DBA28FB5C0047760)</p> | [Google Drive](https://drive.google.com/open?id=1eHM3l4fMo6DsQYGmey7UZGiTmQquHw25) | [Yiming Cui](https://github.com/ymcui)      | [github](https://github.com/ymcui/Chinese-BERT-wwm) | é€šç”¨ |
| RBTL4 | 4å±‚large | [è®¯é£äº‘-e8dN](http://pan.iflytek.com/link/7B04C5BF09812DB241BBA973D649824C) |  | [Yiming Cui](https://github.com/ymcui)      | [github](https://github.com/ymcui/Chinese-BERT-wwm) | é€šç”¨ |
| RBTL6 | 6å±‚large | [è®¯é£äº‘-XNMA](http://pan.iflytek.com/link/B935B1F701A8FD352CAA74614126C4A2) |  | [Yiming Cui](https://github.com/ymcui)      | [github](https://github.com/ymcui/Chinese-BERT-wwm) | é€šç”¨ |
| RoBERTa-wwm-ext  |base |  <p>[Google Drive](https://drive.google.com/open?id=1jMAKIJmPn7kADgD3yQZhpsqM-IRM1qZt)<br>[è®¯é£äº‘-Xe1p](http://pan.iflytek.com/#/link/98D11FAAF0F0DBCB094EE19CCDBC98BF)</p> | [Google Drive](https://drive.google.com/open?id=1eHM3l4fMo6DsQYGmey7UZGiTmQquHw25) | [Yiming Cui](https://github.com/ymcui)      | [github](https://github.com/ymcui/Chinese-BERT-wwm) | é€šç”¨ |
| RoBERTa-wwm-ext-large  | large | <p>[Google Drive](https://drive.google.com/open?id=1dtad0FFzG11CBsawu8hvwwzU2R0FDI94)<br>[è®¯é£äº‘-u6gC](http://pan.iflytek.com/#/link/AC056611607108F33A744A0F56D0F6BE)</p> | [Google Drive](https://drive.google.com/open?id=1-2vEZfIFCdM1-vJ3GD6DlSyKT4eVXMKq) | [Yiming Cui](https://github.com/ymcui)      | [github](https://github.com/ymcui/Chinese-BERT-wwm) | é€šç”¨ |
| RoBERTa-base | base |  <p>[Google Drive](https://drive.google.com/open?id=1ykENKV7dIFAqRRQbZIh0mSb7Vjc2MeFA)<br>[ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1hAs7-VSn5HZWxBHQMHKkrg)</p> | <p>[Google Drive](https://drive.google.com/open?id=1H6f4tYlGXgug1DdhYzQVBuwIGAkAflwB)<br>[ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1AGC76N7pZOzWuo8ua1AZfw)</p> | [brightmart](https://github.com/brightmart) | [github](https://github.com/brightmart/roberta_zh) | é€šç”¨ |
| RoBERTa-Large | large |  <p>[Google Drive](https://drive.google.com/open?id=1W3WgPJWGVKlU9wpUYsdZuurAIFKvrl_Y)<br>[ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1Rk_QWqd7-wBTwycr91bmug)</p> | [Google Drive](https://drive.google.com/open?id=1yK_P8VhWZtdgzaG0gJ3zUGOKWODitKXZ) | [brightmart](https://github.com/brightmart) | [github](https://github.com/brightmart/roberta_zh) | é€šç”¨ |
| RoBERTa-tiny | tiny |[huggingface](https://huggingface.co/uer) | [huggingface](https://huggingface.co/uer) | [DBIIR @ RUC](https://github.com/dbiir) | [UER](https://github.com/dbiir/UER-py) | é€šç”¨ |
| RoBERTa-mini | mini |[huggingface](https://huggingface.co/uer) | [huggingface](https://huggingface.co/uer) | [DBIIR @ RUC](https://github.com/dbiir) | [UER](https://github.com/dbiir/UER-py) | é€šç”¨ |
| RoBERTa-small | small |[huggingface](https://huggingface.co/uer) | [huggingface](https://huggingface.co/uer) | [DBIIR @ RUC](https://github.com/dbiir) | [UER](https://github.com/dbiir/UER-py) | é€šç”¨ |
| RoBERTa-medium | medium |[huggingface](https://huggingface.co/uer) | [huggingface](https://huggingface.co/uer) | [DBIIR @ RUC](https://github.com/dbiir) | [UER](https://github.com/dbiir/UER-py) | é€šç”¨ |
| RoBERTa-base | base |[huggingface](https://huggingface.co/uer) | [huggingface](https://huggingface.co/uer) | [DBIIR @ RUC](https://github.com/dbiir) | [UER](https://github.com/dbiir/UER-py) | é€šç”¨ |

### ALBERT

+ 2019 | ALBERT: A Lite BERT For Self-Supervised Learning Of Language Representations | Zhenzhong Lan, et al. | arXiv | [`PDF`](https://arxiv.org/pdf/1909.11942.pdf)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| Albert_tiny  | tiny  |  [Google Drive](https://storage.googleapis.com/albert_zh/albert_tiny_489k.zip) | [Google Drive](https://drive.google.com/open?id=1VBsUJ7R5eWF1VcUBQY6BEn1a9miEvlBr) | [brightmart](https://github.com/brightmart) | [github](https://github.com/brightmart/albert_zh) | é€šç”¨  |
| Albert_base_zh  | base  |  [Google Drive](https://storage.googleapis.com/albert_zh/albert_base_zh_additional_36k_steps.zip) | [Google Drive](https://drive.google.com/open?id=1HeijHGubWR-ElFnfxUf8IrRx7Ghm1S_Q) | [brightmart](https://github.com/brightmart) | [github](https://github.com/brightmart/albert_zh) | é€šç”¨  |
| Albert_large_zh  | large |  [Google Drive](https://storage.googleapis.com/albert_zh/albert_large_zh.zip) | [Google Drive](https://drive.google.com/open?id=1TAuv7OiFN8qbkT6S_VbfVbhkhg2GUF3q) | [brightmart](https://github.com/brightmart)           | [github](https://github.com/brightmart/albert_zh) | é€šç”¨ |
| Albert_xlarge_zh | xlarge |  [Google Drive](https://storage.googleapis.com/albert_zh/albert_xlarge_zh_183k.zip) | [Google Drive](https://drive.google.com/open?id=1kMhogQRX0uGWIGdNhm7-3hsmHlrzY_gp) | [brightmart](https://github.com/brightmart)           | [github](https://github.com/brightmart/albert_zh) | é€šç”¨ |
| Albert_base  | base | [Google Drive](https://storage.googleapis.com/albert_models/albert_base_zh.tar.gz) | | [Google Research](https://github.com/google-research) | [github](https://github.com/google-research/ALBERT)  | é€šç”¨  |
| Albert_large  | large |  [Google Drive](https://storage.googleapis.com/albert_models/albert_large_zh.tar.gz) | | [Google Research](https://github.com/google-research) | [github](https://github.com/google-research/ALBERT)  | é€šç”¨ |
| Albert_xlarge | xlarge  |  [Google Drive](https://storage.googleapis.com/albert_models/albert_xlarge_zh.tar.gz) |  | [Google Research](https://github.com/google-research) | [github](https://github.com/google-research/ALBERT)  | é€šç”¨ |
| Albert_xxlarge | xxlarge |  [Google Drive](https://storage.googleapis.com/albert_models/albert_xxlarge_zh.tar.gz) |  | [Google Research](https://github.com/google-research) | [github](https://github.com/google-research/ALBERT)  | é€šç”¨  |

### NEZHA

+ 2019 | NEZHA: Neural Contextualized Representation for Chinese Language Understanding | Junqiu Wei, et al. | arXiv | [`PDF`](https://arxiv.org/abs/1909.00204)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| NEZHA-base  | base | <p>[Google Drive](https://drive.google.com/drive/folders/1tFs-wMoXIY8zganI2hQgDBoDPqA8pSmh?usp=sharing)<br>[ç™¾åº¦ç½‘ç›˜-ntn3](https://pan.baidu.com/s/1UVQjy9v_Sv4cQd1ELdjqww)</p> | [lonePatient](https://github.com/lonePatient/NeZha_Chinese_PyTorch) | [HUAWEI](https://github.com/huawei-noah)  | [github](https://github.com/huawei-noah/Pretrained-Language-Model) | é€šç”¨|
| NEZHA-base-wwm  |  base | <p>[Google Drive](https://drive.google.com/drive/folders/1bK6WbqAG-B6BX2d9RPprnh2MPK6zL0t_?usp=sharing)<br>[ç™¾åº¦ç½‘ç›˜-f68o](https://pan.baidu.com/s/1-YG8e5V2zKCnR3azsGZT1w)</p> | [lonePatient](https://github.com/lonePatient/NeZha_Chinese_PyTorch) | [HUAWEI](https://github.com/huawei-noah) | [github](https://github.com/huawei-noah/Pretrained-Language-Model) | é€šç”¨ |
| NEZHA-large  |  large  |<p>[Google Drive](https://drive.google.com/drive/folders/1ZPPM5XtTTOrS_CDRak1t2nCBU-LFZ_zs?usp=sharing)<br>[ç™¾åº¦ç½‘ç›˜-7thu](https://pan.baidu.com/s/1R1Ew-Lu8oIP6QhWO6nqp5Q)</p> | [lonePatient](https://github.com/lonePatient/NeZha_Chinese_PyTorch) | [HUAWEI](https://github.com/huawei-noah) | [github](https://github.com/huawei-noah/Pretrained-Language-Model) | é€šç”¨ |
| NEZHA-large-wwm | large  | <p>[Google Drive](https://drive.google.com/drive/folders/1LOAUc9LXyogC2gmP_q1ojqj41Ez01aga?usp=sharing)<br>[ç™¾åº¦ç½‘ç›˜-ni4o](https://pan.baidu.com/s/1JK1RLIJd2wpuypku3stt8w)</p> | [lonePatient](https://github.com/lonePatient/NeZha_Chinese_PyTorch) | [HUAWEI](https://github.com/huawei-noah) | [github](https://github.com/huawei-noah/Pretrained-Language-Model) | é€šç”¨ |
| <p>WoNEZHA</br>(word-base)</p> | base |[ç™¾åº¦ç½‘ç›˜-qgkq](https://pan.baidu.com/s/1ABKwUuIiMEEsRXxxlbyKmw) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/WoBERT) | é€šç”¨ |

### MacBERT

+ 2020 | Revisiting Pre-Trained Models for Chinese Natural Language Processing | Yiming Cui, et al. | arXiv | [`PDF`](https://arxiv.org/pdf/2004.13922.pdf)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| MacBERT-base  | base  | <p>[Google Drive](https://drive.google.com/file/d/1aV69OhYzIwj_hn-kO1RiBa-m8QAusQ5b/view?usp=sharing)<br>[è®¯é£äº‘-E2cP](http://pan.iflytek.com/#/link/CF2A1F9AEBF859650E8956854A994C1B)</p> |         | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/MacBERT) | é€šç”¨ |
| MacBERT-large | large  | <p>[Google Drive](https://drive.google.com/file/d/1lWYxnk1EqTA2Q20_IShxBrCPc5VSDCkT/view?usp=sharing)<br>[è®¯é£äº‘-3Yg3](http://pan.iflytek.com/#/link/805D743F3826EC4F4EB5C774D34432AE)</p> |         | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/MacBERT) | é€šç”¨ |


### WoBERT

+ 2020 | æé€Ÿä¸æ‰ç‚¹ï¼šåŸºäºè¯é¢—ç²’åº¦çš„ä¸­æ–‡WoBERT | è‹å‰‘æ—. | spaces | [`Blog post`](https://kexue.fm/archives/7758)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| WoBERT | base |[ç™¾åº¦ç½‘ç›˜-kim2](https://pan.baidu.com/s/1BrdFSx9_n1q2uWBiQrpalw) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/WoBERT) | é€šç”¨ |
| WoBERT-plus | base |[ç™¾åº¦ç½‘ç›˜-aedw](https://pan.baidu.com/s/1Ltq3ltQsyBCj56zoOOvI9A) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/WoBERT) | é€šç”¨ |

### XLNET

+ 2019 | XLNet: Generalized Autoregressive Pretraining for Language Understanding | Zhilin Yang, et al. | arXiv | [`PDF`](https://arxiv.org/abs/1906.08237)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| XLNet-base    | base |<p>[Google Drive](https://drive.google.com/open?id=1m9t-a4gKimbkP5rqGXXsEAEPhJSZ8tvx)<br>[è®¯é£äº‘-uCpe](http://pan.iflytek.com/#/link/32619C31BDEFAF2D82CB8C7F66F01D5C)</p> | [Google Drive](https://drive.google.com/open?id=1mPDgcMfpqAf2wk9Nl8OaMj654pYrWXaR) | [Yiming Cui](https://github.com/ymcui)      | [github](https://github.com/ymcui/Chinese-XLNet) | é€šç”¨     |
| XLNet-mid  | middle | <p>[Google Drive](https://drive.google.com/open?id=1342uBc7ZmQwV6Hm6eUIN_OnBSz1LcvfA)<br>[è®¯é£äº‘-68En](http://pan.iflytek.com/#/link/ED7DF7ED04B871AFE8E4D97704B9134D)</p> | [Google Drive](https://drive.google.com/open?id=1u-UmsJGy5wkXgbNK4w9uRnC0RxHLXhxy) | [Yiming Cui](https://github.com/ymcui)      | [github](https://github.com/ymcui/Chinese-XLNet) | é€šç”¨ |
| XLNet_zh_Large |  large | [ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1dy0Z27DoZdMpSmoz1Q4G5A)  |   | [brightmart](https://github.com/brightmart) | [github](https://github.com/brightmart/xlnet_zh) | é€šç”¨  |

### ELECTRA

+ 2020 | ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators | Kevin Clark, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2003.10555)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| ELECTRA-180g-large    | large | <p>[Google Drive](https://drive.google.com/file/d/1P9yAuW0-HR7WvZ2r2weTnx3slo6f5u9q/view?usp=sharing)<br>[è®¯é£äº‘-Yfcy](http://pan.iflytek.com/#/link/7605874F5A11CD693C60EAB79005CCF3)</p> |         | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/Chinese-ELECTRA) | é€šç”¨ |
| ELECTRA-180g-small-ex | small | <p>[Google Drive](https://drive.google.com/file/d/1NYJTKH1dWzrIBi86VSUK-Ml9Dsso_kuf/view?usp=sharing)<br>[è®¯é£äº‘-GUdp](http://pan.iflytek.com/#/link/3EFCF909FC5CFEA6F0EA7AA774C64CF0)</p> |         | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/Chinese-ELECTRA) | é€šç”¨ |
| ELECTRA-180g-base  |  base | <p>[Google Drive](https://drive.google.com/file/d/1RlmfBgyEwKVBFagafYvJgyCGuj7cTHfh/view?usp=sharing)<br>[è®¯é£äº‘-Xcvm](http://pan.iflytek.com/#/link/38E14C9BDBE8E93F09DFE2198E308489)</p> |         | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/Chinese-ELECTRA) | é€šç”¨ |
| ELECTRA-180g-small    | small  |  <p>[Google Drive](https://drive.google.com/file/d/177EVNTQpH2BRW-35-0LNLjV86MuDnEmu/view?usp=sharing)<br>[è®¯é£äº‘-qsHj](http://pan.iflytek.com/#/link/D1B8FE678FA5BC31AA43BD99AD09913E)</p> |         | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/Chinese-ELECTRA) | é€šç”¨ |
| legal-ELECTRA-large   |  large  |  <p>[Google Drive](https://drive.google.com/file/d/1jPyVi_t4QmTkFy7PD-m-hG-lQ8cIETzD/view?usp=sharing)<br>[è®¯é£äº‘-7f7b](http://pan.iflytek.com/#/link/CC111ED9B1D4AE7E26C69A520A6D8759)</p> |         | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/Chinese-ELECTRA) | å¸æ³•é¢†åŸŸ |
| legal-ELECTRA-base    | base   | <p>[Google Drive](https://drive.google.com/file/d/12ZLaoFgpqGJxSi_9KiQV-jdVN4XRGMiD/view?usp=sharing)<br>[è®¯é£äº‘-7f7b](http://pan.iflytek.com/#/link/CC111ED9B1D4AE7E26C69A520A6D8759)</p> |         | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/Chinese-ELECTRA) | å¸æ³•é¢†åŸŸ |
| legal-ELECTRA-small   |  small   |  <p>[Google Drive](https://drive.google.com/file/d/1arQ5qNTNoc1OyMH8wBUKdTMy2QponIFY/view?usp=sharing)<br>[è®¯é£äº‘-7f7b](http://pan.iflytek.com/#/link/CC111ED9B1D4AE7E26C69A520A6D8759)</p> |         | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/Chinese-ELECTRA) | å¸æ³•é¢†åŸŸ |
| ELECTRA-tiny  |  tiny   | <p>[Google Drive](https://drive.google.com/file/d/1UP4byt4-kgenwST0KvyMYNbln6FfaSLp/view?usp=sharing)<br>[ç™¾åº¦ç½‘ç›˜-rs99](https://pan.baidu.com/share/init?surl=4b-IiCkjRg-6XIYPXnezZA)</p> |         | [CLUE](https://github.com/CLUEbenchmark) | [github](https://github.com/CLUEbenchmark/ELECTRA) | é€šç”¨ |

### ZEN

+ 2019 | ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations | Shizhe Diao, et al. | arXiv | [`PDF`](https://arxiv.org/pdf/1911.00720.pdf)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | 
| ZEN-Base |  base  |  | <p>[Google Drive](https://drive.google.com/open?id=1oxNdYMQOpFe3QlttH98bAqg_FQiiVeMr)<br>[ç™¾åº¦ç½‘ç›˜](https://pan.baidu.com/s/1E2ylFnzGSkwBc8tY_OqZYg)</p> | [Sinovation Ventures AI Institute](https://github.com/sinovation) | [github](https://github.com/sinovation/ZEN) | é€šç”¨ |

### ERNIE

+ 2019 | ERNIE: Enhanced Representation through Knowledge Integration | Yu Sun, et al. | arXiv | [`PDF`](https://arxiv.org/abs/1904.09223)

+ 2020 | SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis | Hao Tian, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2005.05635)

+ 2020 | ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding | Dongling Xiao, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2010.12148)

| æ¨¡å‹ | ç‰ˆæœ¬ | PaddlePaddle | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| ernie-1.0-base  | base   |  [link](https://ernie-github.cdn.bcebos.com/model-ernie1.0.1.tar.gz) |  | [PaddlePaddle](https://github.com/PaddlePaddle) | [github](https://github.com/PaddlePaddle/ERNIE) | é€šç”¨  |
| ernie_1.0_skep_large | large | [link](https://senta.bj.bcebos.com/skep/ernie_1.0_skep_large_ch.tar.gz) |    | [Baidu](https://github.com/baidu) | [github](https://github.com/baidu/Senta)  | æƒ…æ„Ÿåˆ†æ |
| ernie-gram | base | [link](https://ernie-github.cdn.bcebos.com/model-ernie-gram-zh.1.tar.gz) |    | [Baidu](https://github.com/baidu) | [github](https://github.com/PaddlePaddle/ERNIE/tree/develop/ernie-gram)  | é€šç”¨ |

å¤‡æ³¨: 

> PaddlePaddleè½¬TensorFlowå¯å‚è€ƒ: [tensorflow_ernie](https://github.com/ArthurRizar/tensorflow_ernie)

> PaddlePaddleè½¬PyTorchå¯å‚è€ƒ: [ERNIE-Pytorch](https://github.com/nghuyong/ERNIE-Pytorch)

### ERNIE3

+ 2021 | ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation | Yu Sun, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2107.02137)

+ 2021 | ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation | Shuohuan Wang, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2106.02241)

| æ¨¡å‹ | ç‰ˆæœ¬ | PaddlePaddle | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| ernie-3.0-base  | 12-layer, 768-hidden, 12-heads   |  [link](https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh.pdparams) |  | [PaddlePaddle](https://github.com/PaddlePaddle) | [github](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-3.0) | é€šç”¨  |
| ernie-3.0-medium  | 6-layer, 768-hidden, 12-heads   |  [link](https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_medium_zh.pdparams) |  | [PaddlePaddle](https://github.com/PaddlePaddle) | [github](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-3.0) | é€šç”¨  |
| ernie-3.0-mini  | 6-layer, 384-hidden, 12-heads  |  [link](https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_mini_zh.pdparams) |  | [PaddlePaddle](https://github.com/PaddlePaddle) | [github](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-3.0) | é€šç”¨  |
| ernie-3.0-micro  | 4-layer, 384-hidden, 12-heads  |  [link](https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_micro_zh.pdparams) |  | [PaddlePaddle](https://github.com/PaddlePaddle) | [github](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-3.0) | é€šç”¨  |
| ernie-3.0-nano  | 4-layer, 312-hidden, 12-heads   |  [link](https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_nano_zh.pdparams) |  | [PaddlePaddle](https://github.com/PaddlePaddle) | [github](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-3.0) | é€šç”¨  |


### RoFormer

+ 2021 | RoFormer: Enhanced Transformer with Rotary Position Embedding | Jianlin Su, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2104.09864)

+ 2021 | Transformerå‡çº§ä¹‹è·¯ï¼š2ã€åšé‡‡ä¼—é•¿çš„æ—‹è½¬å¼ä½ç½®ç¼–ç  | è‹å‰‘æ—. | spaces | [`Blog post`](https://kexue.fm/archives/8265)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| roformer  |  base(L12) | [ç™¾åº¦ç½‘ç›˜-xy9x](https://pan.baidu.com/s/1fiss862YsGCwf2HvU_Jm-g) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/roformer) | é€šç”¨  |
| roformer  |  small(L6) | [ç™¾åº¦ç½‘ç›˜-gy97](https://pan.baidu.com/s/1iIXgZHHCgrYGXVRRSSCVPg) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/roformer) | é€šç”¨  |
| roformer-char  |  base(L12) | [ç™¾åº¦ç½‘ç›˜-bt94](https://pan.baidu.com/s/1Q1pq8F4Fsl6bTipUAkqeDQ) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/roformer) | é€šç”¨  |
| roformerV2  |  small(L6) | [ç™¾åº¦ç½‘ç›˜-ttn4](https://pan.baidu.com/s/1huUrC9P60Afggo8AfiUcmA)[è¿½ä¸€](https://open.zhuiyi.ai/releases/nlp/models/zhuiyi/chinese_roformer-v2-char_L-6_H-384_A-6.zip) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/roformer-v2) | é€šç”¨  |
| roformerV2  |  base(L12) | [ç™¾åº¦ç½‘ç›˜-pfoh](https://pan.baidu.com/s/1qcnN4LVKVe0-mnHlkN3-6Q)[è¿½ä¸€](https://open.zhuiyi.ai/releases/nlp/models/zhuiyi/chinese_roformer-v2-char_L-12_H-768_A-12.zip) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/roformer-v2) | é€šç”¨  |
| roformerV2  |  large(L24) | [ç™¾åº¦ç½‘ç›˜-npfv](https://pan.baidu.com/s/1QiJWSZrGxn8vek-8myvL6w)[è¿½ä¸€](https://open.zhuiyi.ai/releases/nlp/models/zhuiyi/chinese_roformer-v2-char_L-24_H-1024_A-16.zip) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/roformer-v2) | é€šç”¨  |


### StructBERT

+ 2019 | StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding | Wei Wang, et al. | arXiv | [`PDF`](https://arxiv.org/abs/1908.04577)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| StructBERT  |  large(L24) | | [é˜¿é‡Œäº‘](https://alice-open.oss-cn-zhangjiakou.aliyuncs.com/StructBERT/ch_model) | [Alibaba](https://github.com/alibaba) | [github](https://github.com/alibaba/AliceMind/tree/main/StructBERT) | é€šç”¨  |

### Lattice-BERT

+ 2021 | Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models | Yuxuan Lai, et al. | arXiv | [`PDF`](https://arxiv.org/pdf/2104.07204.pdf)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| LatticeBERT  |  tiny(L4) | | [é˜¿é‡Œäº‘](https://alice-open.oss-cn-zhangjiakou.aliyuncs.com/LatticeBERT/chinese_labert-tiny-std-512.tar.gz) | [Alibaba](https://github.com/alibaba) | [github](https://github.com/alibaba/AliceMind/tree/main/LatticeBERT) | é€šç”¨  |
| LatticeBERT  |  small(L6) | | [é˜¿é‡Œäº‘](https://alice-open.oss-cn-zhangjiakou.aliyuncs.com/LatticeBERT/chinese_labert-lite-std-512.tar.gz) | [Alibaba](https://github.com/alibaba) | [github](https://github.com/alibaba/AliceMind/tree/main/LatticeBERT) | é€šç”¨  |
| LatticeBERT  |  base(L12) | | [é˜¿é‡Œäº‘](https://alice-open.oss-cn-zhangjiakou.aliyuncs.com/LatticeBERT/chinese_labert-base-std-512.tar.gz) | [Alibaba](https://github.com/alibaba) | [github](https://github.com/alibaba/AliceMind/tree/main/LatticeBERT) | é€šç”¨  |

### Mengzi-BERT

+ 2021 | Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese | Zhuosheng Zhang, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2110.06696)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| Mengzi-BERT  |  base(L12) | | [huggingface](https://huggingface.co/Langboat/mengzi-bert-base) | [Langboat](https://github.com/Langboat) | [github](https://github.com/Langboat/Mengzi) | é€šç”¨  |
| Mengzi-BERT-fin  |  base(L12) | | [huggingface](https://huggingface.co/Langboat/mengzi-bert-base-fin) | [Langboat](https://github.com/Langboat) | [github](https://github.com/Langboat/Mengzi) | é‡‘èè´¢ç»  |
| bloom-6b4-zh  |  6B(L30) | | [huggingface](https://huggingface.co/Langboat/bloom-6b4-zh) | [Langboat](https://github.com/Langboat) | [github](-) | é€šç”¨  |

### TaCL

+ 2021 | TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning | Yixuan Su, et al. | arXiv | [`PDF`](https://arxiv.org/pdf/2111.04198.pdf)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| TaCL  |  base(L12) | | [huggingface](https://huggingface.co/cambridgeltl/tacl-bert-base-chinese) | [yxuansu](https://github.com/yxuansu) | [github](https://github.com/yxuansu/TaCL) | é€šç”¨  |

### MC-BERT

+ 2021 | MC-BERT: Conceptualized Representation Learning for Chinese Biomedical Text Mining | alibaba-research | arXiv | [`PDF`](https://arxiv.org/pdf/2008.10813.pdf)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| MC-BERT  |  base(L12) | | [link](https://drive.google.com/open?id=1ccXRvaeox5XCNP_aSk_ttLBY695Erlok) | [alibaba-research](https://github.com/alibaba-research) | [github](https://github.com/alibaba-research/ChineseBLUE) | ç”Ÿç‰©åŒ»ç–—  |

### äºŒéƒç¥

| æ¨¡å‹ | ç‰ˆæœ¬ |ç±»å‹| TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |---- |
| Erlangshen  |  large(L24) | bert | | [huggingface](https://huggingface.co/IDEA-CCNL/Erlangshen-1.3B) | [IDEA-CCNL](https://github.com/IDEA-CCNL) | [github](https://github.com/IDEA-CCNL/Fengshenbang-LM) | ä¸­æ–‡é€šç”¨  |

### PERT

+ 2022 | PERT: Pre-Training BERT with Permuted Language Model | Yiming Cui, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2203.06906)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| PERT-base  | base(12L)  | [ç™¾åº¦ç½‘ç›˜-rcsw](https://pan.baidu.com/s/1yDHkYKmdaJkliTGHWQtdFA?pwd=rcsw)| [huggingface](https://huggingface.co/hfl/chinese-pert-base) | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/PERT) | é€šç”¨ |
| PERT-large | large(24L)  | [ç™¾åº¦ç½‘ç›˜-e9hs](https://pan.baidu.com/s/1MG44TRIgqV6m_StfB_yBqQ?pwd=e9hs) | [huggingface](https://huggingface.co/hfl/chinese-pert-large)  | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/PERT) | é€šç”¨ |

### MobileBERT

+ 2020 | MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices | Zhiqing Sun, et al. | arXiv | [`PDF`](https://arxiv.org/pdf/2004.02984.pdf)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| Chinese-MobileBERT-base-f2  | base  | [ç™¾åº¦ç½‘ç›˜-56bj](https://pan.baidu.com/s/16g1LgXXAV01I-cFgPdeOow?pwd=56bj)|  | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/Chinese-MobileBERT) | é€šç”¨ |
| Chinese-MobileBERT-base-f4  | base  | [ç™¾åº¦ç½‘ç›˜-v2v7](https://pan.baidu.com/s/16SGBJhWFYru47EEyTZJljA?pwd=v2v7)|  | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/Chinese-MobileBERT) | é€šç”¨ |
| Chinese-MobileBERT-large-f2  | large  | [ç™¾åº¦ç½‘ç›˜-6m5a](https://pan.baidu.com/s/1Kp7n8lQJOtevzMovKSa3kw?pwd=6m5a)|  | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/Chinese-MobileBERT) | é€šç”¨ |
| Chinese-MobileBERT-large-f4  | large  | [ç™¾åº¦ç½‘ç›˜-3h9b](https://pan.baidu.com/s/19xz9kH1HmM2Og0Aqn7l6vA?pwd=3h9b)|  | [Yiming Cui](https://github.com/ymcui) | [github](https://github.com/ymcui/Chinese-MobileBERT) | é€šç”¨ |

### GAU-Î±

+ 2022 | GAU-Î±: (FLASH) Transformer Quality in Linear Time | Weizhe Hua, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2202.10447.pdf) | [`blog`](https://spaces.ac.cn/archives/9052)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| chinese_GAU-alpha-char_L-24_H-768  | base  | [ä¸‹è½½](https://open.zhuiyi.ai/releases/nlp/models/zhuiyi/chinese_GAU-alpha-char_L-24_H-768.zip)|  | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology/GAU-alpha) | [github](https://github.com/ymcui/Chinese-MobileBERT) | é€šç”¨ |


## NLGç³»åˆ—

### GPT

+ 2019 | Improving Language Understandingby Generative Pre-Training | Alec Radford, et al. | arXiv | [`PDF`](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

+ 2019 | Language Models are Unsupervised Multitask Learners | Alec Radford, et al. | arXiv | [`PDF`](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| GPT2  | 30äº¿è¯­æ–™  |  | <p>[Google Drive](https://drive.google.com/file/d/1mT_qCQg4AWnAXTwKfsyyRWCRpgPrBJS3)<br>[ç™¾åº¦ç½‘ç›˜-ffz6](https://pan.baidu.com/s/1yiuTHXUr2DpyBqmFYLJH6A)</p> | [Caspar ZHANG](https://github.com/imcaspar) | [gpt2-ml](https://github.com/imcaspar/gpt2-ml) | é€šç”¨ |
| GPT2 | 15äº¿è¯­æ–™  |  | <p>[Google Drive](https://drive.google.com/file/d/1IzWpQ6I2IgfV7CldZvFJnZ9byNDZdO4n)<br>[ç™¾åº¦ç½‘ç›˜-q9vr](https://pan.baidu.com/s/1TA_3e-u2bXg_hcx_NwVbGw)</p> | [Caspar ZHANG](https://github.com/imcaspar)  | [gpt2-ml](https://github.com/imcaspar/gpt2-ml)  |  é€šç”¨ |
| CDial-GPTLCCC-base  | base |  | [huggingface](https://huggingface.co/thu-coai/CDial-GPT_LCCC-base) | [thu-coai](https://github.com/thu-coai)         | [CDial-GPT](https://github.com/thu-coai/CDial-GPT)       |  ä¸­æ–‡å¯¹è¯  |
| CDial-GPT2LCCC-base | base |  | [huggingface](https://huggingface.co/thu-coai/CDial-GPT2_LCCC-base) | [thu-coai](https://github.com/thu-coai)| [CDial-GPT](https://github.com/thu-coai/CDial-GPT)           |  ä¸­æ–‡å¯¹è¯ |
| CDial-GPTLCCC-large | large |  | [huggingface](https://huggingface.co/thu-coai/CDial-GPT_LCCC-large) | [thu-coai](https://github.com/thu-coai)         | [CDial-GPT](https://github.com/thu-coai/CDial-GPT)           | ä¸­æ–‡å¯¹è¯ |
| GPT2-dialogue       |  base    |  | <p>[Google Drive](https://drive.google.com/drive/folders/1Ogz3eapvtvdY4VUcY9AEwMbNRivLKhri?usp=sharing)</br>[ç™¾åº¦ç½‘ç›˜-osi6](https://pan.baidu.com/s/1qDZ24VKLBU9GKARX9Ev65g)</p> | [yangjianxin1](https://github.com/yangjianxin1) | [GPT2-chitchat](https://github.com/yangjianxin1/GPT2-chitchat) |  é—²èŠå¯¹è¯ |
| GPT2-mmi  | base  |   | <p>[Google Drive](https://drive.google.com/drive/folders/1oWgKXP6VG_sT_2VMrm0xL4uOqfYwzgUP?usp=sharing)</br>[ç™¾åº¦ç½‘ç›˜-1j88](https://pan.baidu.com/s/1ubXGuEvY8KmwEjIVTJVLww)</p> | [yangjianxin1](https://github.com/yangjianxin1) | [GPT2-chitchat](https://github.com/yangjianxin1/GPT2-chitchat) |  é—²èŠå¯¹è¯ |
| GPT2-æ•£æ–‡æ¨¡å‹   | base |   | <p>[Google Drive](https://drive.google.com/drive/folders/1rJC4niJKMVwixUQkuL9k5teLRnEYTmUf?usp=sharing)</br>[ç™¾åº¦ç½‘ç›˜-fpyu](https://pan.baidu.com/s/1nbrW5iw34GRhoTin8uU2tQ)</p> | [Zeyao Du](https://github.com/Morizeyao)        | [GPT2-Chinese](https://github.com/Morizeyao/GPT2-Chinese)    |   æ•£æ–‡ |
| GPT2-è¯—è¯æ¨¡å‹ |  base  |    | <p>[Google Drive](https://drive.google.com/drive/folders/1Z6nF1nrgTkrZcRLHedQHXb4_M9I7yQPN?usp=sharing)</br>[ç™¾åº¦ç½‘ç›˜-7fev](https://pan.baidu.com/s/1Hy0OQ5xZcTLer9MQZW8o3g)</p> | [Zeyao Du](https://github.com/Morizeyao) | [GPT2-Chinese](https://github.com/Morizeyao/GPT2-Chinese)    |  è¯—è¯  |
| GPT2-å¯¹è”æ¨¡å‹  |  base   |  | <p>[Google Drive](https://drive.google.com/drive/folders/1ZnsvS7oHRVueNKj_SeEhiQt86aze3ojj?usp=sharing)</br>[ç™¾åº¦ç½‘ç›˜-i5n0](https://pan.baidu.com/s/1j9yVQwjlXZq58wOyXK4lcg)</p> | [Zeyao Du](https://github.com/Morizeyao)        | [GPT2-Chinese](https://github.com/Morizeyao/GPT2-Chinese)    | å¯¹è” |
| roformer-gpt  |  base(L12) | [ç™¾åº¦ç½‘ç›˜-2nnn](https://pan.baidu.com/s/11YTnWLX0ThQr2P2yW0P7GA) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/roformer) | é€šç”¨  |

### GPT-3

+ 2019 | Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context | Zihang Dai, et al. | arXiv | [`PDF`](https://arxiv.org/abs/1901.02860)

+ 2020 | Language Models are Few-Shot Learners | Tom B. Brown, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2005.14165)

| æ¨¡å‹ | ç‰ˆæœ¬ | ä»‹ç» | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- |---- |
| Chinese-Transformer-XL |  29äº¿å‚æ•°(GPT-3) | [é¡¹ç›®é¦–é¡µ](https://gpt-3.aminer.cn/) | [æ¨¡å‹ä¸‹è½½](http://dorc-model-team.ks3-cn-beijing.ksyun.com/ren-zhi/my-model/mp_rank_00_model_states.pt) | [THUDM](https://github.com/THUDM) | [github](https://github.com/THUDM/Chinese-Transformer-XL) | é€šç”¨  |

### NEZHA-Gen

+ 2019 | NEZHA: Neural Contextualized Representation for Chinese Language Understanding | Junqiu Wei, et al. | arXiv | [`PDF`](https://arxiv.org/abs/1909.00204)

+ 2019 | Improving Language Understandingby Generative Pre-Training | Alec Radford, et al. | arXiv | [`PDF`](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| NEZHA-Gen  | base | <p>[Google Drive](https://drive.google.com/drive/folders/1i4f_8LhaVDNjnGlLXNJ0rNgBP0E4L6V0?usp=sharing)<br>[ç™¾åº¦ç½‘ç›˜-rb5m](https://pan.baidu.com/s/1Bgle8TpcxHyuUz_jAXOBWw)</p> |  | [HUAWEI](https://github.com/huawei-noah)     | [github](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-Gen-TensorFlow) | é€šç”¨  |
| NEZHA-Gen  | base  | <p>[Google Drive](https://drive.google.com/drive/folders/1B5-jxUlzhoKwFVMQ-nkqqbmJQgr1lRAp?usp=sharing)<br>[ç™¾åº¦ç½‘ç›˜-ytim](https://pan.baidu.com/s/1me6_BGYHbWFdTi80vRQ2Lg)</p> |  | [HUAWEI](https://github.com/huawei-noah)     | [github](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-Gen-TensorFlow) | è¯—æ­Œ |

### CPM-Generate

+ 2020 | CPM: A Large-scale Generative Chinese Pre-trained Language Model | Zhengyan Zhang, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2012.00413)

| æ¨¡å‹ | ç‰ˆæœ¬ | èµ„æº | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- |---- |
| CPM |  26äº¿å‚æ•° | [é¡¹ç›®é¦–é¡µ](https://cpm.baai.ac.cn/) | [æ¨¡å‹ä¸‹è½½](https://cpm.baai.ac.cn/download.html) | [Tsinghua AI](https://github.com/TsinghuaAI) | [github](https://github.com/TsinghuaAI/CPM-Generate) | é€šç”¨  |

å¤‡æ³¨: 

> PyTorchè½¬TensorFlowå¯å‚è€ƒ: [CPM-LM-TF2](https://github.com/qhduan/CPM-LM-TF2)

> PyTorchè½¬PaddlePaddleå¯å‚è€ƒ: [CPM-Generate-Paddle](https://github.com/jm12138/CPM-Generate-Paddle)

### T5

+ 2019 | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | Colin Raffel, et al. | arXiv | [`PDF`](https://arxiv.org/abs/1910.10683)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- |---- |
| T5 | small |[huggingface](https://huggingface.co/uer/t5-small-chinese-cluecorpussmall) | [huggingface](https://huggingface.co/uer/t5-small-chinese-cluecorpussmall) | [DBIIR @ RUC](https://github.com/dbiir) | [UER](https://github.com/dbiir/UER-py) | é€šç”¨ |


### T5-PEGASUS

+ 2019 | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | Colin Raffel, et al. | arXiv | [`PDF`](https://arxiv.org/abs/1910.10683)

+ 2019 | PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization | Jingqing Zhang, et al. | arXiv | [`PDF`](https://arxiv.org/pdf/1912.08777.pdf)

+ 2021 | T5 PEGASUSï¼šå¼€æºä¸€ä¸ªä¸­æ–‡ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹ | è‹å‰‘æ—. | spaces | [`Blog post`](https://spaces.ac.cn/archives/8209)

| æ¨¡å‹ | ç‰ˆæœ¬ | Keras | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- |---- |
| T5 PEGASUS | base |[ç™¾åº¦ç½‘ç›˜-3sfn](https://pan.baidu.com/s/1lQ9Dt9wZDO3IgiCL9tP-Ug) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/t5-pegasus) | é€šç”¨ |
| T5 PEGASUS | small |[ç™¾åº¦ç½‘ç›˜-qguk](https://pan.baidu.com/s/1bXRVWnDyAck9VfSO9_1oJQ) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/t5-pegasus) | é€šç”¨ |

>  Kerasè½¬PyTorchå¯å‚è€ƒ: [t5-pegasus-pytorch](https://github.com/renmada/t5-pegasus-pytorch)

### Mengzi-T5

+ 2021 | Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese | Zhuosheng Zhang, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2110.06696)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| Mengzi-T5  |  base(L12) | | [huggingface](https://huggingface.co/Langboat/mengzi-t5-base) | [Langboat](https://github.com/Langboat) | [github](https://github.com/Langboat/Mengzi) | é€šç”¨  |

### PanGu-Alpha

+ 2021 | PanGu-Î±: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation | Wei Zeng, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2104.12369)

| æ¨¡å‹ | ç‰ˆæœ¬ | èµ„æº | ä¸‹è½½åœ°å€ | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- |---- |
| ç›˜å¤Î±-2.6B |  2.6G | [é¡¹ç›®é¦–é¡µ](https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha/src/branch/master) | [æ¨¡å‹ä¸‹è½½](https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha/src/branch/master) | [PCL-Platform.Intelligence](https://git.openi.org.cn/PCL-Platform.Intelligence) | [github](https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha) | é€šç”¨  |
| ç›˜å¤Î±-13B |  12G | [é¡¹ç›®é¦–é¡µ](https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha/src/branch/master) | [æ¨¡å‹ä¸‹è½½](https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha/src/branch/master) | [PCL-Platform.Intelligence](https://git.openi.org.cn/PCL-Platform.Intelligence) | [github](https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha) | é€šç”¨  |
| ç›˜å¤Î±-2.6B pytorchç‰ˆæœ¬|  2.6G | [é¡¹ç›®é¦–é¡µ](https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha-GPU/src/branch/master/panguAlpha_pytorch) | [æ¨¡å‹ä¸‹è½½](https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha-GPU/src/branch/master/panguAlpha_pytorch#user-content-%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD) | [PCL-Platform.Intelligence](https://git.openi.org.cn/PCL-Platform.Intelligence) | [github](https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha-GPU) | é€šç”¨  |
| ç›˜å¤Î±-13B pytorchç‰ˆæœ¬ |  12G | [é¡¹ç›®é¦–é¡µ](https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha-GPU/src/branch/master/panguAlpha_pytorch) | [æ¨¡å‹ä¸‹è½½](https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha-GPU/src/branch/master/panguAlpha_pytorch#user-content-%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E4%B8%8B%E8%BD%BD) | [PCL-Platform.Intelligence](https://git.openi.org.cn/PCL-Platform.Intelligence) | [github](https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha-GPU) | é€šç”¨  |

### EVA

+ 2021 | EVA: An Open-Domain Chinese Dialogue System with Large-Scale Generative Pre-Training | Hao Zhou, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2108.01547)

| æ¨¡å‹ | ç‰ˆæœ¬ | ä»‹ç» | æ¨¡å‹ä¸‹è½½ | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ | å¤‡æ³¨ |
| ---- | ---- | ---- | ---- | ---- | ---- |---- |---- |
| EVA |  28äº¿å‚æ•° | [é¡¹ç›®é¦–é¡µ](https://wudaoai.cn/model/detail/EVA) | [æ¨¡å‹ä¸‹è½½](https://wudaoai.cn/model/download?resourceId=1428554651225075712&filename=eva-ckpt.tar.gz) | [thu-coai](https://github.com/thu-coai) | [github](https://github.com/thu-coai/EVA) | ä¸­æ–‡å¼€æ”¾åŸŸå¯¹è¯  | éœ€è¦ç™»é™†æ‰èƒ½ä¸‹è½½ |

### BART

+ 2019 | BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | Mike Lewis, et al. | arxiv | [`PDF`](https://arxiv.org/abs/1910.13461)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| BART-base  |  base |  | [huggingface](https://huggingface.co/fnlp/bart-base-chinese)| [fastNLP](https://github.com/fastnlp) | [github](https://github.com/fastnlp/CPT) | ä¸­æ–‡é€šç”¨  |
| BART-large  |  large |  | [huggingface](https://huggingface.co/fnlp/bart-large-chinese)| [fastNLP](https://github.com/fastnlp) | [github](https://github.com/fastnlp/CPT) | ä¸­æ–‡é€šç”¨  |

### é—»ä»²

| æ¨¡å‹ | ç‰ˆæœ¬ |ç±»å‹| TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |---- |
| Wenzhong  |  large(L24) | GPT2 | | [huggingface](https://huggingface.co/IDEA-CCNL/Wenzhong-3.5B) | [IDEA-CCNL](https://github.com/IDEA-CCNL) | [github](https://github.com/IDEA-CCNL/Fengshenbang-LM) | ä¸­æ–‡é€šç”¨  |

### ä½™å…ƒ

| æ¨¡å‹ | ç‰ˆæœ¬ |ç±»å‹| TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |---- |
| Yuyuan |  large(L24) | GPT2 | | [huggingface](https://huggingface.co/IDEA-CCNL/Yuyuan-3.5B) | [IDEA-CCNL](https://github.com/IDEA-CCNL) | [github](https://github.com/IDEA-CCNL/Fengshenbang-LM) | åŒ»å­¦é¢†åŸŸ  |

### RWKV

| æ¨¡å‹ | ç‰ˆæœ¬ |ç±»å‹| TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |---- |
| RWKV |  base(L12) | ç±»ä¼¼GPT-2 | | [github](https://github.com/BlinkDL/AI-Writer/releases) | [PENG Bo](https://github.com/BlinkDL) | [github](https://github.com/BlinkDL/AI-Writer) | å°è¯´  |


## NLU-NLGç³»åˆ—

### UniLM

+ 2019 | Unified Language Model Pre-training for Natural Language Understanding and Generation | Li Dong, et al. | arXiv | [`PDF`](https://arxiv.org/abs/1905.03197)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| Unilm  |  base | [ç™¾åº¦ç½‘ç›˜-tblr](https://pan.baidu.com/s/1HgxIkBl5Yfwrzs1K1B6NFA) | [ç™¾åº¦ç½‘ç›˜-etwf](https://pan.baidu.com/s/1DHJGOFJ5cce5N5g4aBDiMQ) | [YunwenTechnology](https://github.com/YunwenTechnology) | [github](https://github.com/YunwenTechnology/Unilm) | é€šç”¨  |

### Simbert 

+ 2020 | é±¼ä¸ç†ŠæŒå…¼å¾—ï¼šèåˆæ£€ç´¢å’Œç”Ÿæˆçš„SimBERTæ¨¡å‹ | è‹å‰‘æ—. | spaces | [`Blog post`](https://kexue.fm/archives/7427)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| SimBERT Tiny  |  tiny | [ç™¾åº¦ç½‘ç›˜-1tp7](https://pan.baidu.com/s/1z_agqTuBTuyHANwrS-gPcg) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/pretrained-models) | é€šç”¨  |
| SimBERT Small  |  small | [ç™¾åº¦ç½‘ç›˜-nu67](https://pan.baidu.com/s/1kq_EQDI0gpiZBLFd_AxwrA) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/pretrained-models) | é€šç”¨  |
| SimBERT Base  |  base | [ç™¾åº¦ç½‘ç›˜-6xhq](https://pan.baidu.com/s/1uGfQmX1Kxcv_cXTVsvxTsQ) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/pretrained-models) | é€šç”¨  |

### RoFormer-sim

+ 2021 | SimBERTv2æ¥äº†ï¼èåˆæ£€ç´¢å’Œç”Ÿæˆçš„RoFormer-Simæ¨¡å‹ | è‹å‰‘æ—. | spaces | [`Blog post`](https://kexue.fm/archives/8454)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| roformer-sim  |  base(L12) | [ç™¾åº¦ç½‘ç›˜-2cgz](https://pan.baidu.com/s/1f1FB288nv1a6jYjsNCordg) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/roformer-sim) | é€šç”¨  |
| roformer-sim  |  small(L6) | [ç™¾åº¦ç½‘ç›˜-h68q](https://pan.baidu.com/s/1r0eJ7shGwQ0RzV9BTFFW4g) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/roformer-sim) | é€šç”¨  |
| roformer-sim-v2  |  base(L12) | [ç™¾åº¦ç½‘ç›˜-w15n](https://pan.baidu.com/s/1Igh3tSvSu_ahDZmGaOlVoA) | | [ZhuiyiTechnology](https://github.com/ZhuiyiTechnology) | [github](https://github.com/ZhuiyiTechnology/roformer-sim) | é€šç”¨  |

### å‘¨æ–‡ç‹

| æ¨¡å‹ | ç‰ˆæœ¬ |ç±»å‹| TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |---- |
| Zhouwenwang  |  base(L12) | roformer | | [huggingface](https://huggingface.co/IDEA-CCNL/Zhouwenwang-110M) | [IDEA-CCNL](https://github.com/IDEA-CCNL) | [github](https://github.com/IDEA-CCNL/Fengshenbang-LM) | ä¸­æ–‡é€šç”¨  |
| Zhouwenwang  |  large(L24) | roformer | | [huggingface](https://huggingface.co/IDEA-CCNL/Zhouwenwang-1.3B) | [IDEA-CCNL](https://github.com/IDEA-CCNL) | [github](https://github.com/IDEA-CCNL/Fengshenbang-LM) | ä¸­æ–‡é€šç”¨  |

### CPM-2

+ 2021 | CPM-2: Large-scale Cost-effective Pre-trained Language Models | Zhengyan Zhang, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2106.10715)

| æ¨¡å‹ | ç‰ˆæœ¬ | ä»‹ç» | æ¨¡å‹ä¸‹è½½ | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ | å¤‡æ³¨ |
| ---- | ---- | ---- | ---- | ---- | ---- |---- |---- |
| CPM-2 |  110äº¿å‚æ•° | [é¡¹ç›®é¦–é¡µ](https://wudaoai.cn/home) | [æ¨¡å‹ä¸‹è½½](https://resource.wudaoai.cn/home?ind=2&name=WuDao%20WenYuan&id=1394901846484627456) | [BAAI-WuDao](https://github.com/BAAI-WuDao) | [github](https://github.com/BAAI-WuDao/Model) | é€šç”¨  | éœ€è¦ç”³è¯·æ‰èƒ½ä¸‹è½½ |
| CPM-2 |  100äº¿å‚æ•° | [é¡¹ç›®é¦–é¡µ](https://wudaoai.cn/home) | [æ¨¡å‹ä¸‹è½½](https://resource.wudaoai.cn/home?ind=2&name=WuDao%20WenYuan&id=1394901846484627456) | [BAAI-WuDao](https://github.com/BAAI-WuDao) | [github](https://github.com/BAAI-WuDao/Model) | ä¸­è‹±  | éœ€è¦ç”³è¯·æ‰èƒ½ä¸‹è½½ |
| CPM-2 |  1980äº¿å‚æ•° | [é¡¹ç›®é¦–é¡µ](https://wudaoai.cn/home) | [æ¨¡å‹ä¸‹è½½](https://resource.wudaoai.cn/home?ind=2&name=WuDao%20WenYuan&id=1394901846484627456) | [BAAI-WuDao](https://github.com/BAAI-WuDao) | [github](https://github.com/BAAI-WuDao/Model) | ä¸­è‹±  | éœ€è¦ç”³è¯·æ‰èƒ½ä¸‹è½½ |

### CPT

+ 2021 | CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation | Yunfan Shao, et al. | arxiv | [`PDF`](https://arxiv.org/pdf/2109.05729.pdf)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| CPT-base  |  base(L12) |  | [huggingface](https://huggingface.co/fnlp/cpt-base)| [fastNLP](https://github.com/fastnlp) | [github](https://github.com/fastnlp/CPT) | é€šç”¨  |
| CPT-large  |  large(L24) |  | [huggingface](https://huggingface.co/fnlp/cpt-large)| [fastNLP](https://github.com/fastnlp) | [github](https://github.com/fastnlp/CPT) | é€šç”¨  |

### GLM

+ 2022 | GLM: General Language Model Pretraining with Autoregressive Blank Infilling | Zhengxiao Du, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2103.10360)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| GLM  |  large | | [åœ°å€](https://mailstsinghuaeducn-my.sharepoint.com/personal/duzx16_mails_tsinghua_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fduzx16%5Fmails%5Ftsinghua%5Fedu%5Fcn%2FDocuments%2Fmodels&ga=1) | [THUDM](https://github.com/THUDM) | [github](https://github.com/THUDM/glm) | é€šç”¨  |
| GLM  |  xxlarge | | [åœ°å€](https://wudaoai.cn/model/download?resourceId=1420992103650996224&filename=GLM-10B-zh.tar.bz2) | [THUDM](https://github.com/THUDM) | [github](https://github.com/THUDM/glm) | é€šç”¨  |
| GLM-130B  |  130B | | [åœ°å€](https://models.aminer.cn/glm/zh-CN/download/GLM-130B) | [THUDM](https://models.aminer.cn/glm-130b/) | [github](https://github.com/THUDM/GLM-130B) | é€šç”¨  |


## Multi-Modal

### WenLan

+ 2021 | WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training | Yuqi Huo, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2103.06561)

| æ¨¡å‹ | ç‰ˆæœ¬ | ä»‹ç» | æ¨¡å‹ä¸‹è½½ | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ | å¤‡æ³¨ |
| ---- | ---- | ---- | ---- | ---- | ---- |---- |---- |
| BriVL(WenLan) |  10äº¿å‚æ•° | [é¡¹ç›®é¦–é¡µ](https://wudaoai.cn/model/detail/BriVL) | [æ¨¡å‹ä¸‹è½½](https://wudaoai.cn/model/download?resourceId=1425655534320660480&filename=BriVL-1.0-1B-zh.tar) | [BAAI-WuDao](https://github.com/BAAI-WuDao) | [github](https://github.com/BAAI-WuDao/BriVlL) | ä¸­æ–‡é€šç”¨å›¾æ–‡  | éœ€è¦ç™»é™†æ‰èƒ½ä¸‹è½½ |

### CogView

+ 2021 | CogView: Mastering Text-to-Image Generation via Transformers | Ming Ding, et al. | arXiv | [`PDF`](https://arxiv.org/pdf/2105.13290.pdf)

| æ¨¡å‹ | ç‰ˆæœ¬ | ä»‹ç» | æ¨¡å‹ä¸‹è½½ | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ | å¤‡æ³¨ |
| ---- | ---- | ---- | ---- | ---- | ---- |---- |---- |
| CogView |  40äº¿å‚æ•° | [é¡¹ç›®é¦–é¡µ](https://wudaoai.cn/model/detail/CogView) | [æ¨¡å‹ä¸‹è½½](https://wudaoai.cn/model/detail/CogView) | [THUDM ](https://github.com/THUDM) | [github](https://github.com/THUDM/CogView) | ä¸­æ–‡å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ | éœ€è¦ç™»é™†æ‰èƒ½ä¸‹è½½ |

### ç´«ä¸œå¤ªåˆ


| æ¨¡å‹ | ç‰ˆæœ¬ | ä»‹ç» | æ¨¡å‹ä¸‹è½½ | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ | å¤‡æ³¨ |
| ---- | ---- | ---- | ---- | ---- | ---- |---- |---- |
| ç´«ä¸œå¤ªåˆ- light_vision_text   |  | [é¡¹ç›®é¦–é¡µ](https://gitee.com/zidongtaichu/multi-modal-models/tree/master/light_vision_text) | [æ¨¡å‹ä¸‹è½½](https://gitee.com/zidongtaichu/multi-modal-models/tree/master/light_vision_text) | [ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€](https://gitee.com/zidongtaichu) | [github](https://gitee.com/zidongtaichu/multi-modal-models) | ä¸­æ–‡å›¾åƒ-æ–‡æœ¬é¢†åŸŸ | ç´«ä¸œå¤ªåˆå¤šæ¨¡æ€å¤§æ¨¡å‹ä¸­çš„å›¾åƒ-æ–‡æœ¬é¢„è®­ç»ƒæ¨¡å‹ |
| ç´«ä¸œå¤ªåˆ-text[GPT] |  32äº¿å‚æ•° | [é¡¹ç›®é¦–é¡µ](https://gitee.com/zidongtaichu/multi-modal-models/tree/master/text) | [ç™¾åº¦ç½‘ç›˜-nos5](https://pan.baidu.com/s/1Wsu5OVlQBNai24NhNiaqRw) | [ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€](https://gitee.com/zidongtaichu) | [github](https://gitee.com/zidongtaichu/multi-modal-models) | ä¸­æ–‡é€šç”¨ | ç´«ä¸œå¤ªåˆå¤šæ¨¡æ€å¤§æ¨¡å‹ä¸­çš„æ–‡æœ¬é¢„è®­ç»ƒæ¨¡å‹ |
| ç´«ä¸œå¤ªåˆ-vision |   | [é¡¹ç›®é¦–é¡µ](https://gitee.com/zidongtaichu/multi-modal-models/tree/master/vision) | [æ¨¡å‹ä¸‹è½½](https://gitee.com/zidongtaichu/multi-modal-models/tree/master/vision) | [ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€](https://gitee.com/zidongtaichu) | [github](https://gitee.com/zidongtaichu/multi-modal-models) | è§†è§‰é¢†åŸŸ | ç´«ä¸œå¤ªåˆå¤šæ¨¡æ€å¤§æ¨¡å‹ä¸­çš„è§†è§‰é¢„è®­ç»ƒæ¨¡å‹ |
| ç´«ä¸œå¤ªåˆ-speech  |  | [é¡¹ç›®é¦–é¡µ](https://gitee.com/zidongtaichu/multi-modal-models/tree/master/speech) | [æ¨¡å‹ä¸‹è½½](https://gitee.com/zidongtaichu/multi-modal-models/tree/master/speech) | [ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€](https://gitee.com/zidongtaichu) | [github](https://gitee.com/zidongtaichu/multi-modal-models) | è¯­éŸ³é¢†åŸŸ | ç´«ä¸œå¤ªåˆå¤šæ¨¡æ€å¤§æ¨¡å‹ä¸­çš„è¯­éŸ³æ£€æµ‹ä¸è¯†åˆ«å¤šä»»åŠ¡æ¨¡å‹ |

### Mengzi-oscar

+ 2021 | Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese | Zhuosheng Zhang, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2110.06696)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| Mengzi-oscar  |  base(L12) | | [huggingface](https://huggingface.co/Langboat/mengzi-oscar-base) | [Langboat](https://github.com/Langboat) | [github](https://github.com/Langboat/Mengzi) | ä¸­æ–‡å¤šæ¨¡æ€-å›¾æ–‡  |

### R2D2

+ 2022 | Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework | Chunyu Xie, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2205.03860)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | é¦–é¡µ| åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| R2D2ViT-L  |  large | | [Google](https://drive.google.com/file/d/18Fd3vGvj0Dz8rPlxROxugjZaF8Z4jf7g/view) | [yuxie11](https://github.com/yuxie11) | [github](https://github.com/yuxie11/R2D2) |[zero](https://zero.so.com/) |ä¸­æ–‡å¤šæ¨¡æ€-å›¾æ–‡  |
|PRD2ViT-L |  large | | [Google](https://drive.google.com/file/d/15zDdam7_-YT0suA3Wc226vvxcyBxWZ_O/view?usp=sharing) | [yuxie11](https://github.com/yuxie11) | [github](https://github.com/yuxie11/R2D2) |[zero](https://zero.so.com/) |ä¸­æ–‡å¤šæ¨¡æ€-å›¾æ–‡  |

### Chinese-CLIP

+ 2021 | Learning Transferable Visual Models From Natural Language Supervision | Alec Radford, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2103.00020)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| Chinese-CLIP  |  base | | [aliyuncs](https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/checkpoints/clip_cn_vit-b-16.pt) | [OFA Sys](https://github.com/OFA-Sys) | [github](https://github.com/OFA-Sys/Chinese-CLIP) | ä¸­æ–‡å¤šæ¨¡æ€-å›¾æ–‡  |

## Table

### SDCUP

+ 2021 | Improving Text-to-SQL with Schema Dependency Learning | Binyuan Hui, et al. | arXiv | [`PDF`](https://arxiv.org/abs/2103.04399)

| æ¨¡å‹ | ç‰ˆæœ¬ | TensorFlow | PyTorch | ä½œè€…| æºåœ°å€ | åº”ç”¨é¢†åŸŸ |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| sdcup  |  base | | [é˜¿é‡Œäº‘](http://alice-open.oss-cn-zhangjiakou.aliyuncs.com/SDCUP/sdcup_base_model.bin-50000) | [Alibaba](https://github.com/alibaba) | [github](https://github.com/alibaba/AliceMind/tree/main/SDCUP) | ä¸­æ–‡è¡¨æ ¼  |
| sdcup  |  large | | [é˜¿é‡Œäº‘](http://alice-open.oss-cn-zhangjiakou.aliyuncs.com/SDCUP/sdcup_large_model.bin-60000) | [Alibaba](https://github.com/alibaba) | [github](https://github.com/alibaba/AliceMind/tree/main/SDCUP) | ä¸­æ–‡è¡¨æ ¼  |

## æ›´æ–°

* 2022.09.11 å¢åŠ [bloom-6b4](#Mengzi-BERT),å¤šè¯­è¨€é¢„è®­ç»ƒbloomç³»åˆ—ç”Ÿæˆæ¨¡å‹7b1å‚æ•°(https://huggingface.co/bigscience/bloom-7b1 )çš„ä¸­æ–‡vocabæå–ï¼Œbloomç³»åˆ—å¦æœ‰æœ€å¤§176Bæ¨¡å‹(https://huggingface.co/bigscience/bloom ).
* 2022.09.11 å¢åŠ [GLM-130B](#GLM),æå‡ºäº†å¼€æºçš„åŒè¯­é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹ GLM(General Language Model)ã€‚
* 2022.09.11 å¢åŠ [PanGu-Î±: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation ](#PanGu-Alpha) 2.6Bå’Œ13B ç”Ÿæˆæ¨¡å‹pytorchç‰ˆ
* 2022.07.10 å¢åŠ [Chinese-CLIP](#Chinese-CLIP),CLIPæ¨¡å‹çš„ä¸­æ–‡ç‰ˆæœ¬ï¼Œä½¿ç”¨å¤§è§„æ¨¡ä¸­æ–‡æ•°æ®è¿›è¡Œè®­ç»ƒï¼ˆ~2äº¿å›¾æ–‡å¯¹ï¼‰ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·å®ç°ä¸­æ–‡é¢†åŸŸçš„è·¨æ¨¡æ€æ£€ç´¢ã€å›¾åƒè¡¨ç¤ºç­‰.
* 2022.06.29 å¢åŠ [ERNIE 3.0](#ERNIE3),å¤§è§„æ¨¡çŸ¥è¯†å¢å¼ºé¢„è®­ç»ƒè¯­è¨€ç†è§£å’Œç”Ÿæˆ.
* 2022.06.22 å¢åŠ [Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework](#R2D2)ï¼ŒåŸºäºå¤§è§„æ¨¡ä¸­æ–‡è·¨æ¨¡æ€åŸºå‡†æ•°æ®é›†Zeroï¼Œè®­ç»ƒè§†è§‰è¯­è¨€é¢„è®­ç»ƒæ¡†æ¶ R2D2ï¼Œç”¨äºå¤§è§„æ¨¡è·¨æ¨¡æ€å­¦ä¹ ã€‚
* 2022.06.15 å¢åŠ [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](#GLM),æå‡ºäº†ä¸€ç§æ–°çš„é€šç”¨è¯­è¨€æ¨¡å‹ GLM(General Language Model)ã€‚ ä½¿ç”¨è‡ªå›å½’å¡«ç©ºç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒï¼Œå¯ä»¥é’ˆå¯¹å„ç§è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚
* 2022.05.16 å¢åŠ [GAU-Î±](#GAU-Î±),ä¸»è¦æå‡ºäº†ä¸€ä¸ªèåˆäº†Attentionå±‚å’ŒFFNå±‚çš„æ–°è®¾è®¡GAUï¼ˆGated Attention Unitï¼Œé—¨æ§æ³¨æ„åŠ›å•å…ƒï¼‰ï¼Œå®ƒæ˜¯æ–°æ¨¡å‹æ›´å¿«ã€æ›´çœã€æ›´å¥½çš„å…³é”®ï¼Œæ­¤å¤–å®ƒä½¿å¾—æ•´ä¸ªæ¨¡å‹åªæœ‰ä¸€ç§å±‚ï¼Œä¹Ÿæ˜¾å¾—æ›´ä¸ºä¼˜é›…ã€‚
* 2022.03.27 å¢åŠ [RoFormer-V2](#RoFormer),RoFormerå‡çº§ç‰ˆï¼Œä¸»è¦é€šè¿‡ç»“æ„çš„ç®€åŒ–æ¥æå‡é€Ÿåº¦ï¼Œå¹¶é€šè¿‡æ— ç›‘ç£é¢„è®­ç»ƒå’Œæœ‰ç›‘ç£é¢„è®­ç»ƒçš„ç»“åˆæ¥æå‡æ•ˆæœï¼Œä»è€Œè¾¾åˆ°äº†é€Ÿåº¦ä¸æ•ˆæœçš„â€œåŒèµ¢â€ã€‚
* 2022.03.02 å¢åŠ [MobileBERT](#MobileBERT),MobileBERTæ˜¯BERT-largeæ¨¡å‹æ›´â€œè‹—æ¡â€çš„ç‰ˆæœ¬ï¼Œä½¿ç”¨äº†ç“¶é¢ˆç»“æ„ï¼ˆbottleneckï¼‰å¹¶ä¸”å¯¹è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç¥ç»ç½‘ç»œä¹‹é—´çš„å¹³è¡¡åšäº†ç»†è‡´çš„è®¾è®¡ã€‚
* 2022.02.24 å¢åŠ [PERT: Pre-Training BERT with Permuted Language Model](#PERT),ä¸€ç§åŸºäºä¹±åºè¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆPERTï¼‰ï¼Œåœ¨ä¸å¼•å…¥æ©ç æ ‡è®°[MASK]çš„æƒ…å†µä¸‹è‡ªç›‘ç£åœ°å­¦ä¹ æ–‡æœ¬è¯­ä¹‰ä¿¡æ¯ã€‚
* 2021.12.06 å¢åŠ [SDCUP: Improving Text-to-SQL with Schema Dependency Learning](#SDCUP),è¾¾æ‘©é™¢æ·±åº¦è¯­è¨€æ¨¡å‹ä½“ç³» AliceMind å‘å¸ƒä¸­æ–‡ç¤¾åŒºé¦–ä¸ªè¡¨æ ¼é¢„è®­ç»ƒæ¨¡å‹ SDCUPã€‚
* 2021.11.27 å¢åŠ [RWKV](#RWKV)ä¸­æ–‡é¢„è®­ç»ƒç”Ÿæˆæ¨¡å‹,ç±»ä¼¼ GPT-2,æ¨¡å‹å‚è€ƒåœ°å€ï¼š[RWKV-LM](https://github.com/BlinkDL/RWKV-LM)
* 2021.11.27 å¢åŠ IDEAç ”ç©¶é™¢å¼€æºçš„å°ç¥æ¦œç³»åˆ—è¯­è¨€æ¨¡å‹ï¼ŒåŒ…å«[äºŒéƒç¥](#äºŒéƒç¥)ã€[å‘¨æ–‡ç‹](#å‘¨æ–‡ç‹)ã€[é—»ä»²](#é—»ä»²)ã€[ä½™å…ƒ](#ä½™å…ƒ)ã€‚
* 2021.11.25 å¢åŠ [MC-BERT: Conceptualized Representation Learning for Chinese Biomedical Text Mining](#MC-BERT), ç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹.
* 2021.11.24 å¢åŠ [TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning](#TaCL), Token-awareå¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒæ¨¡å‹.
* 2021.10.18 å¢åŠ [Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese](#Mengzi-BERT),åŸºäºè¯­è¨€å­¦ä¿¡æ¯èå…¥å’Œè®­ç»ƒåŠ é€Ÿç­‰æ–¹æ³•ç ”å‘äº† Mengzi ç³»åˆ—æ¨¡å‹.
* 2021.10.14 å¢åŠ [ä¸­æ–‡ç‰ˆBART](#BART),è®­ç»ƒæ¯”è¾ƒå¯é çš„ä¸­æ–‡ç‰ˆBARTï¼Œä¸ºä¸­æ–‡ç”Ÿæˆç±»ä»»åŠ¡å¦‚æ‘˜è¦ç­‰æä¾›Baseline.
* 2021.10.14 å¢åŠ [CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation](#CPT),CPTï¼šå…¼é¡¾ç†è§£å’Œç”Ÿæˆçš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹.
* 2021.10.13 å¢åŠ [ç´«ä¸œå¤ªåˆå¤šæ¨¡æ€å¤§æ¨¡å‹](#ç´«ä¸œå¤ªåˆ): å…¨çƒé¦–ä¸ªå¤šæ¨¡æ€å›¾æ–‡éŸ³é¢„è®­ç»ƒæ¨¡å‹,å®ç°äº†è§†è§‰-æ–‡æœ¬-è¯­éŸ³ä¸‰æ¨¡æ€ç»Ÿä¸€è¡¨ç¤ºï¼Œæ„å»ºäº†ä¸‰æ¨¡æ€é¢„è®­ç»ƒå¤§æ¨¡å‹ã€‚
* 2021.09.19 å¢åŠ [CogView: Mastering Text-to-Image Generation via Transformers](#CogView),ä¸–ç•Œæœ€å¤§çš„ä¸­æ–‡å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹,æ¨¡å‹æ”¯æŒæ–‡ç”Ÿæˆå›¾ä¸ºåŸºç¡€çš„å¤šé¢†åŸŸä¸‹æ¸¸ä»»åŠ¡.
* 2021.09.10 å¢åŠ [WenLan: Bridging Vision and Language by Large-Scale Multi-Modal Pre-Training](#WenLan)ï¼Œé¦–ä¸ªä¸­æ–‡é€šç”¨å›¾æ–‡å¤šæ¨¡æ€å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ã€‚
* 2021.09.10 å¢åŠ [EVA: An Open-Domain Chinese Dialogue System with Large-Scale Generative Pre-Training](#EVA)ï¼Œä¸€ä¸ªå¼€æ”¾é¢†åŸŸçš„ä¸­æ–‡å¯¹è¯é¢„è®­ç»ƒæ¨¡å‹ã€‚
* 2021.08.19 å¢åŠ [Chinese-Transformer-XL](#GPT-3)ï¼šåŸºäºä¸­æ–‡é¢„è®­ç»ƒè¯­æ–™WuDaoCorpusï¼ˆ290Gï¼‰è®­ç»ƒçš„GPT-3æ¨¡å‹ã€‚
* 2021.08.16 å¢åŠ [CPM-2: Large-scale Cost-effective Pre-trained Language Models](#CPM-2)
* 2021.08.16 å¢åŠ [Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models](#Lattice-BERT)
* 2021.07.19 å¢åŠ [roformer-sim-v2](#RoFormer-sim)ï¼šåˆ©ç”¨æ ‡æ³¨æ•°æ®å¢å¼ºç‰ˆæœ¬
* 2021.07.15 å¢åŠ [BERT-CCPoem](#BERT)ï¼šå¤å…¸è¯—æ­Œè¯­æ–™è®­ç»ƒçš„BERT
* 2021.07.06 å¢åŠ [ChineseBERTï¼šChinese Pretraining Enhanced by Glyph and Pinyin Information](#BERT)
* 2021.06.22 å¢åŠ [StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding](#StructBERT)
* 2021.06.14 å¢åŠ [RoFormerï¼šEnhanced Transformer with Rotary Position Embedding](#RoFormer)
* 2021.05.25 å¢åŠ [ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding ]((#ERNIE))
* 2021.04.28 å¢åŠ [PanGu-Î±: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation ](#PanGu-Alpha)
* 2021.03.16 å¢åŠ [T5-PEGASUS: å¼€æºä¸€ä¸ªä¸­æ–‡ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹](#T5-PEGASUS)
* 2021.03.09 å¢åŠ UERç³»åˆ—æ¨¡å‹
* 2021.03.04 å¢åŠ [WoBERT: åŸºäºè¯é¢—ç²’åº¦çš„ä¸­æ–‡](#WoBERT)
* 2020.11.11 åˆå§‹åŒ–BERTç³»åˆ—æ¨¡å‹[BERT](#BERT)
